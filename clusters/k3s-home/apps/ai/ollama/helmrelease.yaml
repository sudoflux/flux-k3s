apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: ollama
  namespace: ai
spec:
  interval: 5m
  chart:
    spec:
      chart: app-template
      version: 3.7.3
      sourceRef:
        kind: HelmRepository
        name: bjw-s
        namespace: flux-system
      reconcileStrategy: ChartVersion
  values:
    controllers:
      main:
        enabled: true
        type: deployment
        replicas: 1
        pod:
          nodeSelector:
            kubernetes.io/hostname: k3s3  # Pin to GPU node
        containers:
          main:
            enabled: true
            image:
              repository: ollama/ollama
              tag: latest
            env:
              - name: OLLAMA_HOST
                value: "0.0.0.0"  # Listen on all interfaces
              - name: OLLAMA_MODELS
                value: "/models"
              - name: NVIDIA_VISIBLE_DEVICES
                value: "all"
              - name: NVIDIA_DRIVER_CAPABILITIES
                value: "compute,utility"
            resources:
              requests:
                cpu: 2000m
                memory: 8Gi
                nvidia.com/gpu: 1
              limits:
                cpu: 8000m
                memory: 32Gi
                nvidia.com/gpu: 1
            securityContext:
              privileged: true  # Required for GPU access
    service:
      main:
        controller: main
        ports:
          http:
            port: 11434
            targetPort: 11434
            protocol: TCP
    persistence:
      models:
        enabled: true
        existingClaim: ollama-models-pvc
        globalMounts:
          - path: /models
      cache:
        enabled: true
        type: emptyDir
        sizeLimit: 10Gi
        globalMounts:
          - path: /root/.ollama  # Ollama cache directory